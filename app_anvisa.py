# app_anvisa.py
# æœ¬åœ° ANVISA æˆåˆ†è¡¥å‰‚æˆæƒæŸ¥è¯¢å·¥å…·ï¼ˆæ‰¹é‡æŸ¥è¯¢ + çºµå‘è¯¦æƒ…ï¼‰
# åŠŸèƒ½ï¼šè§„èŒƒåŒ– + åˆ«åæ˜ å°„ + CAS æ”¯æŒ + ä¸­æ–‡è¡¨å¤´ + æ‰¹é‡æŸ¥è¯¢ + Tabs + å¯¼å‡º Excel

import re
import io
import os
import pandas as pd
from unidecode import unidecode
import streamlit as st

# ===================== 1. é…ç½®åŒºåŸŸ =====================

DB_PATH = "anvisa_final_v10.xlsx"
ALIAS_PATH = "anvisa_alias_total.xlsx"

st.set_page_config(
    page_title="Anvisa åˆè§„æŸ¥è¯¢å·¥å…·ï¼ˆæœ¬åœ°ç‰ˆï¼‰",
    layout="wide",
    page_icon="ğŸ‡§ğŸ‡·",
)

# ===================== 2. æ–‡æœ¬è§„èŒƒåŒ–å‡½æ•° =====================

def normalize(text: str) -> str:
    """ç»Ÿä¸€è§„èŒƒåŒ–æˆåˆ†åå­— / åˆ«åï¼Œç”¨äºåŒ¹é…ï¼ˆå»é‡éŸ³ + å°å†™ + å»å¤šä½™ç©ºæ ¼ï¼‰ã€‚"""
    if text is None:
        return ""

    text = str(text)
    text = re.sub(r"\s+", " ", text)
    text = text.strip().strip('"').strip("'")
    text = text.replace(" / ", "/").replace(" /", "/").replace("/ ", "/")
    for ch in ["â€“", "â€”", "âˆ’"]:
        text = text.replace(ch, "-")
    text = unidecode(text)
    text = text.lower()
    return text


# ===================== 3. æ•°æ®åŠ è½½ =====================

@st.cache_data
def load_db(db_path: str) -> pd.DataFrame | None:
    if not os.path.exists(db_path):
        return None
    df = pd.read_excel(db_path)

    if "Ingredient (æˆåˆ†)" not in df.columns:
        st.error("åœ¨ä¸»æ•°æ®åº“é‡Œæ‰¾ä¸åˆ°åˆ—ï¼š'Ingredient (æˆåˆ†)'ï¼Œè¯·æ£€æŸ¥åˆ—åæˆ– Excel æ–‡ä»¶ã€‚")
        st.stop()

    df["__norm_ingredient"] = df["Ingredient (æˆåˆ†)"].apply(normalize)

    if "CAS" in df.columns:
        df["CAS"] = df["CAS"].astype(str)
        df["__norm_cas"] = df["CAS"].apply(normalize)

    return df


@st.cache_data
def load_alias(alias_path: str) -> pd.DataFrame:
    if not os.path.exists(alias_path):
        return pd.DataFrame(columns=["Alias", "Official", "__norm_alias", "__norm_official"])

    alias_df = pd.read_excel(alias_path)
    required_cols = {"Alias", "Official"}
    if not required_cols.issubset(alias_df.columns):
        return pd.DataFrame(columns=["Alias", "Official", "__norm_alias", "__norm_official"])

    alias_df["Alias"] = alias_df["Alias"].astype(str)
    alias_df["Official"] = alias_df["Official"].astype(str)
    alias_df["__norm_alias"] = alias_df["Alias"].apply(normalize)
    alias_df["__norm_official"] = alias_df["Official"].apply(normalize)
    return alias_df


# ===================== 4. æŸ¥è¯¢é€»è¾‘ =====================

def search_ingredients(df: pd.DataFrame, alias_df: pd.DataFrame, query: str) -> pd.DataFrame:
    norm_q = normalize(query)
    if not norm_q:
        return df.iloc[0:0].copy()

    mask_ing = df["__norm_ingredient"].str.contains(norm_q, na=False)

    if "__norm_cas" in df.columns:
        mask_cas = df["__norm_cas"].str.contains(norm_q, na=False)
    else:
        mask_cas = False

    if not alias_df.empty:
        alias_hits = alias_df[alias_df["__norm_alias"].str.contains(norm_q, na=False)]
        target_official_norms = alias_hits["__norm_official"].unique()
        if len(target_official_norms) > 0:
            mask_alias = df["__norm_ingredient"].isin(target_official_norms)
        else:
            mask_alias = False
    else:
        mask_alias = False

    final_mask = mask_ing | mask_cas | mask_alias
    result = df[final_mask].copy()
    return result


# ===================== 5. å…¨å±€è¡¨æ ¼æ ·å¼ï¼ˆç»™çºµå‘ st.table ç”¨ï¼‰ =====================

st.markdown(
    """
    <style>
    /* è®© st.table æ’‘æ»¡å®¹å™¨å®½åº¦ï¼Œé•¿å†…å®¹åœ¨å•å…ƒæ ¼å†…è‡ªåŠ¨æ¢è¡Œ */
    div[data-testid="stTable"] table {
        width: 100%;
        table-layout: auto;
        border-collapse: collapse;
    }

    /* é€šç”¨å•å…ƒæ ¼æ ·å¼ï¼šå·¦å¯¹é½ + è‡ªåŠ¨æ¢è¡Œ */
    div[data-testid="stTable"] thead tr th,
    div[data-testid="stTable"] tbody tr td {
        text-align: left !important;
        vertical-align: middle !important;
        white-space: normal !important;
        word-break: break-word !important;
        overflow-wrap: break-word !important;
        padding: 0.5rem 0.75rem;
    }

    /* æŠŠç¬¬äºŒåˆ—ï¼ˆâ€œå­—æ®µâ€è¿™ä¸€åˆ—ï¼‰è®¾å®½ä¸€ç‚¹ï¼Œå¹¶ä¸”ä¸æ‹†è¡Œ */
    div[data-testid="stTable"] thead tr th:nth-child(2),
    div[data-testid="stTable"] tbody tr td:nth-child(2) {
        width: 140px;
        min-width: 140px;
        white-space: nowrap !important;
    }

    /* æ§åˆ¶è¡¨æ ¼é‡Œæ‰€æœ‰æ–‡å­—å­—å· */
    div[data-testid="stTable"] * {
        font-size: 14px !important;
    }
    </style>
    """,
    unsafe_allow_html=True,
)


# ===================== 6. é¡µé¢ UI å¸ƒå±€ =====================

st.title("ğŸ‡§ğŸ‡· å·´è¥¿ Anvisa ä¿å¥å“æˆåˆ†åˆè§„æŸ¥è¯¢")
st.markdown("---")

df = load_db(DB_PATH)
alias_df = load_alias(ALIAS_PATH)

with st.sidebar:
    st.header("ğŸ“Š æ•°æ®åº“çŠ¶æ€")
    if df is not None:
        st.success(f"âœ… å·²åŠ è½½æ•°æ®åº“: {DB_PATH}")

        if "Ingredient (æˆåˆ†)" in df.columns:
            st.markdown("**ä¸»è¦æˆåˆ†åˆ—:** Ingredient (æˆåˆ†)")

    else:
        st.error("âŒ æœªæ‰¾åˆ°ä¸»æ•°æ®åº“æ–‡ä»¶ï¼è¯·ç¡®ä¿ anvisa_final_v10.xlsx åœ¨åŒç›®å½•ä¸‹ã€‚")

    st.markdown("---")
    st.header("ğŸ’¡ ä½¿ç”¨è¯´æ˜")
    st.markdown(
        """
        1. åœ¨å³ä¾§è¾“å…¥æ¡†**ä¸€è¡Œä¸€ä¸ª**æˆåˆ†åç§°ã€‚  
        2. æ”¯æŒï¼šè‘¡è¯­ã€è‹±æ–‡ã€æ‹‰ä¸æ–‡ã€ä¸­æ–‡ã€CAS å·ã€‚  
        3. **æ¨¡ç³Šæœç´¢ + å»é‡éŸ³**ï¼šè¾“å…¥ `Cafeina` ä¹Ÿèƒ½åŒ¹é… `CafeÃ­na`ã€‚  
        4. æŸ¥è¯¢ç»“æœæ”¯æŒå¯¼å‡ºä¸º Excelã€‚
        """
    )

if df is None:
    st.warning("è¯·å…ˆæŠŠ anvisa_final_v10.xlsx æ”¾åˆ°å½“å‰ç›®å½•ï¼Œç„¶åé‡æ–°è¿è¡Œæ­¤å·¥å…·ã€‚")
    st.stop()

st.subheader("ğŸ” æˆåˆ†æ‰¹é‡æŸ¥è¯¢")
input_text = st.text_area(
    "è¯·è¾“å…¥æˆåˆ†åç§°ï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œä¾‹å¦‚ï¼šCafeinaã€Vitamina Cã€Melatonina æˆ–ä¸­æ–‡å / CASï¼‰ï¼š",
    height=150,
)

# ===================== 7. æ‰¹é‡æŸ¥è¯¢ + çºµå‘å±•ç¤º =====================

final_found_df = pd.DataFrame()
results_not_found = []

if st.button("ğŸš€ å¼€å§‹æŸ¥è¯¢", type="primary"):
    if not input_text.strip():
        st.warning("è¯·è¾“å…¥è‡³å°‘ä¸€ä¸ªæˆåˆ†ï¼")
    else:
        user_queries = [line.strip() for line in input_text.split("\n") if line.strip()]

        results_found = []
        results_not_found = []

        progress_bar = st.progress(0.0)

        for idx, query in enumerate(user_queries):
            progress_bar.progress((idx + 1) / len(user_queries))

            matches = search_ingredients(df, alias_df, query)

            if not matches.empty:
                rename_map = {
                    "Ingredient (æˆåˆ†)": "æˆåˆ†",
                    "CAS": "CAS",
                    "Specs (è§„æ ¼)": "è§„æ ¼",
                    "Function (åŠŸèƒ½)": "åŠŸèƒ½",
                    "Claims (å£°ç§°)": "å£°ç§°",
                    "Labeling (æ ‡ç­¾)": "æ ‡ç­¾",
                    "Other (å…¶ä»–)": "å…¶ä»–",
                    "Link (é“¾æ¥)": "é“¾æ¥",
                }

                exist_cols = [c for c in rename_map.keys() if c in matches.columns]
                display_df = matches[exist_cols].copy()
                display_df.rename(columns=rename_map, inplace=True)

                display_df.insert(0, "æŸ¥è¯¢è¯", query)
                display_df.insert(1, "æ˜¯å¦æˆæƒ", "âœ… YES")

                results_found.append(display_df)
            else:
                results_not_found.append(
                    {
                        "æŸ¥è¯¢è¯": query,
                        "æ˜¯å¦æˆæƒ": "âŒ NOï¼ˆæœªåœ¨åº“ä¸­æ‰¾åˆ°ï¼‰",
                        "å»ºè®®": "è¯·æ£€æŸ¥æ‹¼å†™ï¼Œæˆ–åœ¨åˆ«åè¡¨ anvisa_alias_total.xlsx ä¸­è¡¥å……è¯¥å†™æ³•",
                    }
                )

        progress_bar.empty()
        st.markdown("---")

        tab1, tab2 = st.tabs(["âœ… å·²æˆæƒ / æ‰¾åˆ°çš„æˆåˆ†ï¼ˆçºµå‘è¯¦æƒ…ï¼‰", "âŒ æœªæ‰¾åˆ°çš„æˆåˆ†"])

        with tab1:
            if results_found:
                final_found_df = pd.concat(results_found, ignore_index=True)
                st.success(f"å…±åŒ¹é…åˆ° {len(final_found_df)} æ¡ç›¸å…³è®°å½•")

                # ğŸ”½ å¯¹æ¯æ¡è®°å½•ï¼Œè½¬æˆâ€œå­—æ®µ / å†…å®¹â€çš„çºµå‘è¡¨æ ¼
                for i, row in final_found_df.iterrows():
                    st.markdown(
                        f"**ğŸ”¹ æŸ¥è¯¢è¯ï¼š`{row['æŸ¥è¯¢è¯']}` â€”â€” åŒ¹é…æˆåˆ†ï¼š`{row['æˆåˆ†']}`**"
                    )

                    vertical_df = row.to_frame().reset_index()
                    vertical_df.columns = ["å­—æ®µ", "å†…å®¹"]
                    vertical_df = vertical_df.reset_index(drop=True)

                    st.table(vertical_df)
                    st.markdown("---")
            else:
                st.write("æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„å·²æˆæƒæˆåˆ†ã€‚")

        with tab2:
            if results_not_found:
                not_found_df = pd.DataFrame(results_not_found)
                st.error(f"æœ‰ {len(not_found_df)} ä¸ªæŸ¥è¯¢è¯æœªæ‰¾åˆ°åŒ¹é…é¡¹")
                st.dataframe(not_found_df, use_container_width=True)
            else:
                st.write("æ‰€æœ‰æŸ¥è¯¢è¯éƒ½æ‰¾åˆ°äº†åŒ¹é…é¡¹ï¼")

        # ===================== 8. å¯¼å‡º Excel æŠ¥å‘Š =====================

        st.markdown("---")
        st.subheader("ğŸ“¥ å¯¼å‡ºæŸ¥è¯¢ç»“æœ")

        if final_found_df.empty and not results_not_found:
            st.info("å½“å‰æ²¡æœ‰å¯å¯¼å‡ºçš„æ•°æ®ï¼Œè¯·å…ˆæ‰§è¡Œä¸€æ¬¡æŸ¥è¯¢ã€‚")
        else:
            output = io.BytesIO()
            with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
                if not final_found_df.empty:
                    final_found_df.to_excel(writer, sheet_name="å·²æˆæƒæˆåˆ†", index=False)
                if results_not_found:
                    pd.DataFrame(results_not_found).to_excel(
                        writer, sheet_name="æœªæ‰¾åˆ°æˆåˆ†", index=False
                    )
            output.seek(0)

            st.download_button(
                label="ä¸‹è½½æŸ¥è¯¢ç»“æœï¼ˆExcelï¼‰",
                data=output,
                file_name="Anvisa_æŸ¥è¯¢ç»“æœæŠ¥å‘Š.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            )
